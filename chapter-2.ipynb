{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 2 Exercises on Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Will a flexible method work better than an inflexible one?\n",
    "\n",
    "a. sample size is large and predictors are small: Better. With large datasets, you can assume that you can capture real-world data better than smaller datasets. A flexible method may work better under this assumption.\n",
    "\n",
    "b. number of predictors is large and number of observations is small: Worse. You might capture a lot of noise from the large number of predictors and considering your observations is small, you might not have a good sample of real-world data.\n",
    "\n",
    "c. relationship between predictors and response is highly non-linear: Better. Flexible methods can describe non-linear relationships better.\n",
    "\n",
    "d. variance of error terms is high: Worse. You might capture the noise from the high variance. This situation also calls for more predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Classification or Regression? Inference or Prediction? Provide n and p.\n",
    "\n",
    "a. Regression. Inference. n = 500. p = (profit, number of employees, industry)\n",
    "\n",
    "b. Classification. Prediction. n = 20. p = (price charged for the product, marketing budget, competition price, 10 other vars)\n",
    "\n",
    "c. Regression. Prediction. n = 52. p = (% change in US, British, and German markets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Bias-Variance decomposition\n",
    "\n",
    "a. file: ch2-bias-variance.jpg\n",
    "\n",
    "b. Bias describes the inability of the model to capture the complexity of the real world. As flexibility goes up, the bias of the model (for simple explanations or methods) goes down. Variance describes the changes in variables captured in the dataset that is potentially noise. As flexibility goes up, the variance of the model goes up. The training error goes down as you create more flexible methods and eventually will be zero if enough variance of the training dataset is described by the model. The test error goes down as the model predicts better by using flexible methods but goes higher as you capture noise from the training dataset. It always stays higher or equal to the irreducible error. Bayes error stays constant as the predictors may not capture all variables/variance in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}